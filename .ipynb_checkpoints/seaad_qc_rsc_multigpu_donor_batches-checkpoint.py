#!/usr/bin/env python
# seaad_qc_rsc_multigpu_donor_batches.py
# Multi-GPU QC + per-Donor Scrublet (RAPIDS) with lazy H5AD streaming
# ENV: rapids_singlecell

import os, gc, time, warnings
import numpy as np
import pandas as pd

import anndata as ad
import scanpy as sc
import h5py

import dask
import dask.array as da
from dask.distributed import Client
from dask_cuda import LocalCUDACluster

import cupy as cp
from cupyx import scatter_add
from cupyx.scipy import sparse as cpx

import rapids_singlecell as rsc
from rmm.allocators.cupy import rmm_cupy_allocator
import rmm

# -----------------------
# Configuration
# -----------------------
DATA_DIR   = "/mnt/data/seaad_dlpfc"
H5AD_IN    = f"{DATA_DIR}/SEAAD_A9_RNAseq_all-nuclei.2024-02-13.h5ad"
OUT_H5AD   = f"{DATA_DIR}/SEAAD_qc_scrublet_singlets.h5ad"
OUT_ZARR   = f"{DATA_DIR}/SEAAD_qc_singlets.zarr"

DONOR_KEY        = "Donor ID"
ROW_CHUNK        = 4000
TEST_MAX_CELLS   = 50000      # <-- run on 50k cells only
SCRUBLET_PCS     = 15
SCRUBLET_SIMR    = 2.0
SCRUBLET_RATE    = 0.045
PER_DONOR_MAX    = 12000
QC_MIN_GENES     = 200
QC_MAX_PCT_MT    = 8.0
KEEP_GENES_MIN_CELLS = 10
DASK_DASHBOARD   = ":0"
N_WORKERS        = 4

warnings.filterwarnings("ignore", category=FutureWarning)

# -----------------------
# Dask / RMM memory tuning
# -----------------------
os.environ["DASK_RMM_POOL_SIZE"] = "18GB"
os.environ["DASK_DISTRIBUTED__WORKER__MEMORY__TARGET"] = "0.70"
os.environ["DASK_DISTRIBUTED__WORKER__MEMORY__SPILL"] = "0.75"
os.environ["DASK_DISTRIBUTED__WORKER__MEMORY__PAUSE"] = "0.85"
os.environ["DASK_DISTRIBUTED__WORKER__MEMORY__TERMINATE"] = "0.95"

def start_cluster():
    try:
        cluster = LocalCUDACluster(
            n_workers=N_WORKERS,
            threads_per_worker=1,
            protocol="ucx",
            rmm_pool_size="22GB",
            device_memory_limit="0.9",
            local_directory=os.path.join(DATA_DIR, "dask-tmp"),
            dashboard_address=DASK_DASHBOARD,
        )
    except Exception:
        cluster = LocalCUDACluster(
            n_workers=N_WORKERS,
            threads_per_worker=1,
            protocol="tcp",
            rmm_pool_size="22GB",
            device_memory_limit="0.9",
            local_directory=os.path.join(DATA_DIR, "dask-tmp"),
            dashboard_address=DASK_DASHBOARD,
        )
    client = Client(cluster)
    print("‚úÖ Dask dashboard:", client.dashboard_link)
    rmm.reinitialize(managed_memory=True, pool_allocator=True, devices=[0,1,2,3])
    cp.cuda.set_allocator(rmm_cupy_allocator)
    return cluster, client


def wrap_h5ad_x_as_dask(h5ad_path, row_chunk, limit_rows=None):
    adata_b = sc.read_h5ad(h5ad_path, backed="r")
    n_obs, n_vars = adata_b.n_obs, adata_b.n_vars
    if limit_rows is not None:
        n_obs = min(n_obs, int(limit_rows))
    print(f"  ‚Üí backed: {adata_b.n_obs:,} cells √ó {adata_b.n_vars:,}; using first {n_obs:,} rows")

    with h5py.File(h5ad_path, "r") as h5:
        Xgrp = h5["X"]
        shape = tuple(Xgrp.attrs["shape"])

    starts = list(range(0, n_obs, row_chunk))
    blocks = []
    for s in starts:
        e = min(s + row_chunk, n_obs)
        def _load_block(s=s, e=e):
            with h5py.File(h5ad_path, "r") as hh:
                Xg = hh["X"]
                indptr = Xg["indptr"][s : e + 1]
                idx    = Xg["indices"][indptr[0] : indptr[-1]]
                dat    = Xg["data"][indptr[0] : indptr[-1]]
                iptr   = indptr - indptr[0]
                csr = cpx.csr_matrix((cp.asarray(dat), cp.asarray(idx), cp.asarray(iptr)),
                                     shape=(e - s, shape[1]))
                return csr.toarray()
        blk = dask.delayed(_load_block)()
        blocks.append(da.from_delayed(blk, shape=(e - s, shape[1]), dtype=np.float32))
    X_dask = da.concatenate(blocks, axis=0)

    obs = adata_b.obs.iloc[:n_obs].copy()
    var = adata_b.var.copy()
    adata = ad.AnnData(X=X_dask, obs=obs, var=var)
    return adata, adata_b


def dask_qc_filter(adata):
    print("üß™ Running QC (distributed on GPU)‚Ä¶")
    rsc.pp.flag_gene_family(adata=adata, gene_family_name="mt",   gene_family_prefix="MT-")
    rsc.pp.flag_gene_family(adata=adata, gene_family_name="ribo", gene_family_prefix="RPS")
    rsc.pp.flag_gene_family(adata=adata, gene_family_name="hb",   gene_family_prefix="HB")
    rsc.pp.calculate_qc_metrics(adata, qc_vars=["mt", "ribo", "hb"])

    keep_cell = (adata.obs["n_genes_by_counts"] > QC_MIN_GENES) & (
        adata.obs["pct_counts_mt"] < QC_MAX_PCT_MT
    )
    adata = adata[keep_cell, :].copy()
    print(f"   ‚Üí after cell filter: {adata.n_obs:,} cells")

    counts_per_gene = da.count_nonzero(adata.X, axis=0).compute()
    n_cells = counts_per_gene.get() if isinstance(counts_per_gene, cp.ndarray) else np.asarray(counts_per_gene)
    adata.var["n_cells_by_counts"] = n_cells
    keep_genes = n_cells >= KEEP_GENES_MIN_CELLS
    print(f"   ‚Üí keeping {keep_genes.sum():,}/{len(keep_genes):,} genes with ‚â•{KEEP_GENES_MIN_CELLS} cells")
    adata = adata[:, keep_genes].copy()
    print(f"‚úÖ After QC: {adata.n_obs:,} cells √ó {adata.n_vars:,} genes")
    return adata, keep_genes


def cupy_dense_from_any(X_block) -> cp.ndarray:
    if isinstance(X_block, da.Array):
        Xc = X_block.map_blocks(lambda b: cp.asarray(b, dtype=cp.float32), dtype=cp.float32)
        return Xc.compute()
    if isinstance(X_block, cp.ndarray):
        return X_block.astype(cp.float32, copy=False)
    if cpx.isspmatrix(X_block):
        return X_block.toarray()
    if hasattr(X_block, "toarray"):
        return cp.asarray(X_block.toarray(), dtype=cp.float32)
    return cp.asarray(X_block, dtype=cp.float32)


def main():
    t0 = time.time()
    cluster, client = start_cluster()

    adata, adata_b = wrap_h5ad_x_as_dask(H5AD_IN, ROW_CHUNK, limit_rows=TEST_MAX_CELLS)
    rsc.get.anndata_to_GPU(adata)
    ad_qc, gene_keep_mask = dask_qc_filter(adata)

    print(f"\nü§ñ Running Scrublet per donor on '{DONOR_KEY}' ‚Ä¶")
    ad_qc.obs["doublet_score"] = np.nan
    ad_qc.obs["predicted_doublet"] = pd.Series(index=ad_qc.obs_names, dtype=bool)

    donors = ad_qc.obs[DONOR_KEY].astype(str)
    unique_donors = donors.dropna().unique().tolist()
    print(f"   Found {len(unique_donors)} donors in QC-passed cells")

    for dname in unique_donors:
        mask = (donors == dname).values
        n_d = int(mask.sum())
        if n_d == 0:
            continue
        checkpoint = f"{DATA_DIR}/scrublet_{dname}.csv"
        if os.path.exists(checkpoint):
            print(f"‚úÖ Skipping {dname}, already done")
            continue

        print(f"   ‚Üí [{dname}] cells: {n_d:,}")
        if PER_DONOR_MAX is not None and n_d > PER_DONOR_MAX:
            idx = np.flatnonzero(mask)
            sel_idx = np.random.default_rng(17).choice(idx, size=PER_DONOR_MAX, replace=False)
            donor_mask = np.zeros(ad_qc.n_obs, dtype=bool)
            donor_mask[sel_idx] = True
        else:
            donor_mask = mask

        ad_sub = ad_qc[donor_mask, :].copy()
        X_cu = cupy_dense_from_any(ad_sub.X)
        ad_sub.X = X_cu

        rsc.pp.scrublet(
            ad_sub,
            expected_doublet_rate=SCRUBLET_RATE,
            sim_doublet_ratio=SCRUBLET_SIMR,
            n_prin_comps=SCRUBLET_PCS,
            log_transform=False,
            random_state=0,
        )

        ad_sub.obs[["doublet_score", "predicted_doublet"]].to_csv(checkpoint)
        print(f"üíæ Saved checkpoint: {checkpoint}")

        scores = ad_sub.obs["doublet_score"].to_numpy()
        preds  = ad_sub.obs["predicted_doublet"].to_numpy()
        ad_qc.obs.loc[ad_sub.obs_names, "doublet_score"] = scores
        ad_qc.obs.loc[ad_sub.obs_names, "predicted_doublet"] = preds

        del ad_sub
        cp._default_memory_pool.free_all_blocks()
        gc.collect()

    keep_rows = ~ad_qc.obs["predicted_doublet"].fillna(False).to_numpy()
    print(f"\n‚úÖ Singlets after Scrublet: {keep_rows.sum():,}/{ad_qc.n_obs:,}")

    if TEST_MAX_CELLS is not None:
        with dask.config.set(scheduler="threads"):
            X_host = ad_qc.X[keep_rows, :].astype(np.float32).compute()
        out = ad.AnnData(X=X_host, obs=ad_qc.obs.loc[keep_rows].copy(), var=ad_qc.var.copy())
        out.write_h5ad(OUT_H5AD, compression="gzip")
        print(f"üíæ Wrote {OUT_H5AD}")
    else:
        print("üíæ Streaming singlets to Zarr (not shown here for brevity)")

    client.close()
    cluster.close()
    print(f"‚è±Ô∏è Total time: {(time.time() - t0)/60:.1f} min")


if __name__ == "__main__":
    main()
